summary.gbm               package:gbm               R Documentation

_S_u_m_m_a_r_y _o_f _a _g_b_m _o_b_j_e_c_t

_D_e_s_c_r_i_p_t_i_o_n:

     Computes the relative influence of each variable in the gbm
     object.

_U_s_a_g_e:

     ## S3 method for class 'gbm':
     summary(object,
             cBars=length(object$var.names),
             n.trees=object$n.trees,
             plotit=TRUE,
             order=TRUE,
             method=relative.influence,
             normalize=TRUE,
             ...)

_A_r_g_u_m_e_n_t_s:

  object: a 'gbm' object created from an initial call to 'gbm'.

   cBars: the number of bars to plot. If 'order=TRUE' the only the
          variables with the 'cBars' largest relative influence will
          appear in the barplot. If 'order=FALSE' then the first
          'cBars' variables will appear in the plot. In either case,
          the function will return the relative influence of all of the
          variables.

 n.trees: the number of trees used to generate the plot. Only the first
          'n.trees' trees will be used.

  plotit: an indicator as to whether the plot is generated. 

   order: an indicator as to whether the plotted and/or returned
          relative influences are sorted. 

  method: The function used to compute the relative influence.
          'relative.influence' is the default and is the same as that
          described in Friedman (2001). The other current (and
          experimental) choice is 'permutation.test.gbm'. This method
          randomly permutes each predictor variable at a time and
          computes the associated reduction in predictive performance.
          This is similar to the variable importance measures Breiman
          uses for random forests, but 'gbm' currently computes using
          the entire training dataset (not the out-of-bag observations.

normalize: if 'FALSE' then 'summary.gbm' returns the  unnormalized
          influence. 

     ...: other arguments passed to the plot function. 

_D_e_t_a_i_l_s:

     For 'distribution="gaussian"' this returns exactly the reduction 
     of squared error attributable to each variable. For other loss
     functions this  returns the reduction attributeable to each
     varaible in sum of squared error in  predicting the gradient on
     each iteration. It describes the relative influence  of each
     variable in reducing the loss function. See the references below
     for  exact details on the computation.

_V_a_l_u_e:

     Returns a data frame where the first component is the variable
     name and the second is the computed relative influence, normalized
     to sum to 100.

_A_u_t_h_o_r(_s):

     Greg Ridgeway gregr@rand.org

_R_e_f_e_r_e_n_c_e_s:

     J.H. Friedman (2001). "Greedy Function Approximation: A Gradient
     Boosting Machine," Annals of Statistics 29(5):1189-1232.

     L. Breiman (2001). "Random Forests," Available at <URL:
     ftp://ftp.stat.berkeley.edu/pub/users/breiman/randomforest2001.pdf>.

_S_e_e _A_l_s_o:

     'gbm'

