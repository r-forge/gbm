\relax 
\bibstyle{plain}
\citation{Friedman:2001}
\citation{Friedman:2002}
\citation{Ridgeway:1999}
\citation{FreundSchapire:1997}
\citation{Friedman:2001}
\citation{Friedman:2002}
\@writefile{toc}{\contentsline {section}{\numberline {1}Gradient boosting}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Friedman's gradient boosting machine}{1}}
\newlabel{sec:GradientBoostingMachine}{{1.1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Friedman's Gradient Boost algorithm}}{2}}
\newlabel{fig:GradientBoost}{{1}{2}}
\citation{FHT:2000}
\newlabel{NonparametricRegression1}{{4}{3}}
\newlabel{NonparametricRegression2}{{5}{3}}
\newlabel{eq:Friedman1}{{6}{3}}
\newlabel{EQ:Friedman2}{{7}{3}}
\newlabel{eq:Friedman3}{{8}{3}}
\citation{FHT:2000}
\@writefile{toc}{\contentsline {section}{\numberline {2}Improving boosting methods using control of the learning rate, sub-sampling, and a decomposition for interpretation}{4}}
\newlabel{GBMModifications}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Decreasing the learning rate}{4}}
\newlabel{eq:shrinkage}{{10}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Variance reduction using subsampling}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}ANOVA decomposition}{5}}
\newlabel{ANOVAdecomp}{{11}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Relative influence}{5}}
\newlabel{RelInfluence}{{12}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Common user options}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Loss function}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Boosting as implemented in \texttt  {gbm()}}}{6}}
\newlabel{fig:gbm}{{2}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The relationship between shrinkage and number of iterations}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Estimating the optimal number of iterations}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Out-of-sample predictive performance by number of iterations and shrinkage. Smaller values of the shrinkage parameter offer improved predictive performance, but with decreasing marginal improvement.}}{8}}
\newlabel{fig:shrinkViters}{{3}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Out-of-sample predictive performance of four methods of selecting the optimal number of iterations. The vertical axis plots performance relative the best. The boxplots indicate relative performance across thirteen real datasets from the UCI repository. See \texttt  {demo(OOB-reps)}.}}{9}}
\newlabel{fig:oobperf}{{4}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Available distributions}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Gaussian}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}AdaBoost}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Bernoulli}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Laplace}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Quantile regression}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Cox Proportional Hazard}{11}}
\bibcite{FreundSchapire:1997}{1}
\bibcite{Friedman:2001}{2}
\bibcite{Friedman:2002}{3}
\bibcite{FHT:2000}{4}
\bibcite{Ridgeway:1999}{5}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Poisson}{12}}
